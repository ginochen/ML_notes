{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "Hard margin:\n",
    "\n",
    "For linearly separable training data, select parallel two hyperplanes that can maximize the distance between the two classes. From [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine), the two hyperspaces that host the samples for the two classes $y=-1$ and $y=1$ are $w x - b \\le -1$ and $w x - b \\ge 1$, respectively. Therefore multiplying both sides of the inequalities by y results in $y(wx-b) \\ge 1$. The distance between the two hyperplanes are just $\\frac{2}{||w||}$, which we want to maximize, i.e., minimize $||w||$.  The optimization problem becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize $||w||$ subject to $y(wx_i-b) \\ge 1$, for all i-samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss is just the inverse of the hyperplane distance $||w||$, hence do SGD to update (w,b) w.r.t the slope on Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft margin:\n",
    "\n",
    "For linearly inseparable data, the optimization becomes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize $[\\frac{1}{n}\\sum_{i=1}^n max(0,1-y(wx_i-b)] + \\lambda ||w||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term, \"hinge loss\", is obtained by moving the $y(wx_i-b)$ to the r.h.s of the inequality $y(wx_i-b) \\ge 1$. When correctly predicted max$(0,1-y(wx_i-b) \\le 0)==0$, and if incorrectly predicted max$(0,1-y(wx_i-b) \\gt 0) \\gt 0$. Thus the positive hinge loss is reduced by finding the (w,b) through SGD, and maximize the margin just like hard margin does. The optimized (w,b) then gives the best hyperplane when substituted into $wx-b=0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where did that $\\frac{2}{||w||}$ come from?\n",
    "Ever wonder where that distance measure between the two hyperplanes, $\\frac{2}{||w||}$, in [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine) comes from? Here's a quick and dirty proof. \n",
    "\n",
    "\n",
    "Given an arbitrary point $\\hat{x}$ on the hyperplane $w x - b = -1$. We can use the unit vector $\\frac{w}{||w||}$ and a distance $d$, to get to the point $\\hat{x} + d\\frac{w}{||w||}$ on the other plane $w x - b = 1$. By substituting this point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $w(\\hat{x} + d \\frac{w}{||w||}) - b = 1$ \n",
    "\n",
    "> $(w\\hat{x}-b) + d\\frac{w*w}{||w||} = 1$\n",
    "\n",
    "> $-1 + d\\frac{||w||^2}{||w||} = 1$\n",
    "\n",
    "> $d||w|| = 2$\n",
    "\n",
    "> $ d = \\frac{2}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [SVM on Wiki](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [where did that $\\frac{2}{||w||}$ come from?](https://math.stackexchange.com/questions/1305925/why-does-the-svm-margin-is-frac2-mathbfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
