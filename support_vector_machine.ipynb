{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "# [Decision boundary](https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72)\n",
    "Given the two hyperspace $w x - b \\le -1$ and $w x - b \\ge 1$ with margin that separates the two-class samples, $w$ is the perpendicular vector of the two parallel planes. From vector calculus $u^Tv = P_{vu}||u||$, where $P_{vu}$ is the projection length of v on u. Therefore $wx = P_{xw}||w||$. We can rewrite the hyperspace into $P_{xw}||w|| - b \\le -1$ and $P_{xw}||w|| - b \\ge 1$. This has the benefit of visualizing the distance of any given sample $x_i$ to the hyperplane by using the projection length $P_{x_iw}$!!! \n",
    "\n",
    "A dicision boundary with margin that's not maximized gives a small $P_{x_iw}$ for each sample. This requires a large ||w|| to satisfy the $\\le -1$ and $\\ge 1$ hyperspace conditions. In contrast, if the margin is maximized, i.e., decision boundary separates the two-feature samples well, then the ||w|| will be small. This is why the optimization goal is to minimize ||w||. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard margin:\n",
    "\n",
    "For linearly separable training data, select parallel two hyperplanes that can maximize the distance between the two classes. From [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine), the two hyperspaces that host the samples for the two classes $y=-1$ and $y=1$ are $w x - b \\le -1$ and $w x - b \\ge 1$, respectively. Therefore multiplying both sides of the inequalities by y results in $y(wx-b) \\ge 1$. The distance between the two hyperplanes are just $\\frac{2}{||w||}$, which we want to maximize, i.e., minimize $||w||$.  The optimization problem becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize $||w||$ subject to $y(wx^{(i)}-b) \\ge 1$, for all i-samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss is just the inverse of the hyperplane distance $||w||$, hence do SGD to update (w,b) w.r.t the slope on Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft margin:\n",
    "\n",
    "For linearly inseparable data, the optimization becomes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> minimize $[\\frac{1}{n}\\sum_{i=1}^n max(0,1-y(wx^{(i)}-b)] + \\lambda ||w||^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term, \"hinge loss\", is obtained by moving the $y(wx^{(i)}-b)$ to the r.h.s of the inequality $y(wx^{(i)}-b) \\ge 1$. When correctly predicted max$(0,1-y(wx^{(i)}-b) \\le 0)==0$, and if incorrectly predicted max$(0,1-y(wx^{(i)}-b) \\gt 0) \\gt 0$. Thus the positive hinge loss is reduced by finding the (w,b) through SGD, and maximize the margin just like hard margin does. The optimized (w,b) then gives the best hyperplane when substituted into $wx-b=0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where did that $\\frac{2}{||w||}$ come from?\n",
    "Ever wonder where that distance measure between the two hyperplanes, $\\frac{2}{||w||}$, in [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine) comes from? Here's a quick and dirty proof. \n",
    "\n",
    "\n",
    "Given an arbitrary point $\\hat{x}$ on the hyperplane $w x - b = -1$. We can use the unit vector $\\frac{w}{||w||}$ and a distance $d$, to get to the point $\\hat{x} + d\\frac{w}{||w||}$ on the other plane $w x - b = 1$. By substituting this point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $w(\\hat{x} + d \\frac{w}{||w||}) - b = 1$ \n",
    "\n",
    "> $(w\\hat{x}-b) + d\\frac{w*w}{||w||} = 1$\n",
    "\n",
    "> $-1 + d\\frac{||w||^2}{||w||} = 1$\n",
    "\n",
    "> $d||w|| = 2$\n",
    "\n",
    "> $ d = \\frac{2}{||w||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF = kernel = similarity function\n",
    "Radial basis function (RBF), $f(x) = \\exp(\\frac{-||x-x'||^2}{2\\sigma^2})$, is just the Gaussian function applied on feature points to see how close they are. The $x'$ are called \"landmark\" points for training samples to compare with. If very close, i.e., the ||x-x'|| is near zero and at the peak density 1, i.e., two points are very \"similar\". The variance $\\sigma^2$ in Gaussian controls the width of the peak. The $\\gamma = \\frac{1}{\\sigma^2}$, so smaller $\\gamma$ allows farther points to have more similarity. A larger $\\gamma$ gives less weight and less similarity with farther apart points, i.e., Gaussian peak drops fast with distance. \n",
    "\n",
    "For m training samples, $wf^{(i)}(x^{(i)}) = w_0f_0^{(i)}+w_1f_1^{(i)}(x^{(i)})+w_2f_2^{(i)}(x^{(i)})+...+w_nf_m^{(i)}(x^{(i)}) \\ge 0$ predicts $y=1$, and the new feature dimensions $m \\in \\Re^{m+1}$, where the extra dimension is from $f_0=1$. \n",
    "\n",
    "The $f^{(i)} = [f_0^{(i)}, f_1^{(i)}, \\dotsc, f_m^{(i)}]$ is the ith sample kernel operated on the entire training sample. For instance $f_1^{(i)} = k(x^{(i)},x^{(1)})$ takes the kernel of ith and 1st sample, $f_2^{(i)} = k(x^{(i)},x^{(2)})$, and so on for all training samples. So $f^{(i)}$ essentially goes through all training samples with the single ith sample, i.e., see how close ith sample is to the entire training set.\n",
    "\n",
    "That's it!! We now know how Kernel trick works mathematically, so let's move on to the cost function optimization!!\n",
    "\n",
    "From the logistic regression cross entropy cost function for all i samples: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> min$_w$  $-\\frac{1}{m}\\sum_i^m[y^{(i)} \\log p_w(x^{(i)}) + (1-y^{(i)}) \\log(1-p_w(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p_w(x)$ is a \"hypothesis function $h_w$\" to estimate y. For SVM, the $-\\log(h)$ and $-\\log(1-h)$ is approximated by cost functions [cost$_1$ and cost$_0$](http://www.holehouse.org/mlclass/12_Support_Vector_Machines.html) respectively. When $y=1$, cost$_1=0$ if $wf>1$, and $\\gt 0$ if $wf \\lt 1$. Similarily when $y=0$, cost$_0=0$ if $wf \\lt -1$, and 1 if $wf \\gt -1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> min$_w$  $\\frac{1}{m}\\sum_{i=1}^m[y^{(i)} \\text{cost}_1(wf^{(i)}) + (1-y^{(i)}) \\text{cost}_0(wf^{(i)}) ] + \\frac{\\lambda}{2m}\\sum_{j=1}^m w_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the $f^{(i)}$ is the ith sample kernel done on the original feature x, so the last term sum total is m, as oppose to n in logistic regression cost. The reason is $f$ has dimension equal to the number of training samples m, as oppose to logistic regression summing over the number of n features in x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define $C = \\frac{1}{\\lambda}$ and multiply the loss by a constant $\\frac{m}{\\lambda}$, which doesn't change the minimisation results. This gives a new form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $min\\limits_{w}$  C$\\sum_{i=1}^m[y_i \\text{cost}_1(wf_i) + (1-y_i) \\text{cost}_0(wf_i) ] + \\frac{1}{2}\\sum_{j=1}^m w_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that cost_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [SVM on Wiki](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "* [where did that $\\frac{2}{||w||}$ come from?](https://math.stackexchange.com/questions/1305925/why-does-the-svm-margin-is-frac2-mathbfw)\n",
    "* [SVM cost function](http://www.holehouse.org/mlclass/12_Support_Vector_Machines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
