{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Cross Entropy\n",
    "\n",
    "Ever wonder how to relate the two concepts? Here's my explanation...\n",
    "\n",
    "Let's start from logistic regression since softmax is just a multi-category logistic regression.\n",
    "\n",
    "\n",
    "The logistic regression algorithm is trying to:\n",
    "\n",
    "> Find the optimal (w,b) that maximize the sum of cross entropy between the binary class probability (1 and 0) and the predicted class probability (0 to 1), i.e., Loss.\n",
    "\n",
    "# A measure of \"Loss\"...\n",
    "So what does this have to do with cross entropy and where does Bernoulli distribution come in to play? \n",
    "\n",
    "Let's first ask when is Bernoulli distribution $P(k;p) = p^k*(1-p)^(1-k)$ a maximum? Suppose k=1, then P(k=1;p) = p is a maximum when p=1. Similarly $P(k=0;p) = 1-p$ is a maximum when $p=0$. Therefore Bernoulli distribution can be a good measure of \"distance\" between the binary class probability (k) and predicted class probability (p)!!\n",
    "\n",
    "Therefore if we have mutiple sample which we used to measure, then that'll be maximizing the joint Bernoulli distribution $P_1*P_2*P_3*\\dotsc*P_n$.\n",
    "\n",
    "When $n$ is large this probability is very small, hence it's better to take the log of this joint distribution. This is when Bernoulli becomes cross entropy!!!\n",
    "\n",
    "So the optimization becomes:\n",
    "\n",
    "> Find the (w,b) that minimize loss \n",
    "\n",
    "> $L(k,p) = -[\\log(P_1) + \\log(P_2) + ... \\log(P_n)] = \n",
    "  -\\sum_i [k*\\log(p) + (1-k)*\\log(1-p)]_i = \n",
    "  -\\sum_i [\\text{cross_entropy(k and p)}]_i$\n",
    "\n",
    "Tada!! Bernoulli and cross entropy can both measure Loss, but cross entropy uses sum and won't result in computer precision issue!\n",
    "\n",
    "The same applies to softmax!\n",
    "\n",
    "If $k$ and $p$ are close (i.e., class is predicted good!), the cross_entropy is small. Otherwise entropy is negatively large with positively large $L$.\n",
    "\n",
    "We can use SGD on $(w,b)$ to minimize $L(k,p)$ to get logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
