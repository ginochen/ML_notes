{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantage: \n",
    " * Easy to use, invariant to input data scale, i.e., doesn't need to rescale training data like SVM (optimizes faster when using RBF) or DNN (mini-batch normalization has regularization effect, which adds variance and optimizes faster).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Tree ensemble assumes K trees and each tree gives a prediction $f_k$ for $y^{(i)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\hat{y^{(i)}} = \\sum_{k=1}^K f_k(x^{(i)})$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f_k$'s are the k-th regression tree. Notice regression trees are decision trees with real-valued target variables, otherwise called classification trees with categorical targets.\n",
    "\n",
    "So what is the regularization term for a single regression tree optimization problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\Omega = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where T is the number of leaves and $w_j$ are the leave scores. More (less) leaves means more (less) complicated tree. Smaller weights are encouraged since that means prediction is less likely to be affected by small changes.\n",
    "\n",
    "The objective becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obj = $\\sum_{i=1}^n L(y^{(i)},\\hat{y}^{(i)}) + \\sum_k \\Omega(f_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we can not use SGD to optimize $\\hat{y}^{(i)}$ for a regression tree. Therefore an \"additive training/Boosting\" was developed as a solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\hat{y}_0^{(i)} = 0$\n",
    "\n",
    "> $\\hat{y}_1^{(i)} = \\hat{y}_0^{(i)} + f_1(x^{(i)})$\n",
    "\n",
    "> $\\hat{y}_2^{(i)} = \\hat{y}_1^{(i)} + f_2(x^{(i)})$\n",
    "\n",
    "...\n",
    "\n",
    "> $\\hat{y}_t^{(i)} = \\hat{y}_{t-1}^{(i)} + f_{t}(x^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the t-th round objective becomes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obj$_t = \\sum_{i=1}^n L(y^{(i)},\\hat{y}_t^{(i)}) + \\sum_j^t \\Omega(f_j)$\n",
    " = $\\sum_{i=1}^n L(y^{(i)},\\hat{y}_{t-1}^{(i)}+f_t(x^{(i)})) + \\sum_j^t \\Omega(f_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and try to find $f_t(x^{(i)})$ to minimize the Obj$_t$. The 2nd order Taylor expansion on L w.r.t. $\\hat{y}_{t-1}^{(i)}$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obj$_t \\approx \\sum_{i=1}^n [L(y^{(i)},\\hat{y}_{t-1}^{(i)}) + g^{(i)}f_t(x^{(i)})+\\frac{1}{2}h^{(i)}f_t(x^{(i)})^2] + \\sum_j^t \\Omega(f_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $g$ and $h$ are just the 1st and 2nd order derivative of L.\n",
    "\n",
    "If $L = (y^{(i)}-\\hat{y}_t^{(i)})^2$ is just squared loss , then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obj$_t \\approx \\sum_{i=1}^n [2(\\hat{y}_{t-1}^{(i)} - y^{(i)})f_t(x^{(i)}) + f_t(x^{(i)})^2] + \\sum_j^t \\Omega(f_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [XGBoost technical documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "* [XGBoost speech by the creator](https://www.youtube.com/watch?v=Vly8xGnNiWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
